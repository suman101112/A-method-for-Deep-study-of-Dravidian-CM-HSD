{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "finite-defeat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3999 entries, TA_HL101 to TA_TW4528\n",
      "Data columns (total 2 columns):\n",
      " #   Column                                                            Non-Null Count  Dtype \n",
      "---  ------                                                            --------------  ----- \n",
      " 0   Iyaooo Kovam pattutene sothula visatha vachuruvanugale ..iooo...  3999 non-null   object\n",
      " 1   NOT                                                               3999 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 93.7+ KB\n",
      "None\n",
      "{'NOT', 'OFF'}\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 939 entries, 0 to 938\n",
      "Data columns (total 3 columns):\n",
      " #   Column                                                                                     Non-Null Count  Dtype \n",
      "---  ------                                                                                     --------------  ----- \n",
      " 0   TA_TW15946                                                                                 939 non-null    object\n",
      " 1   Take it this thevidiya Kandipa indha page admin Oru Mutual Punda Vijay fan ha Dhan erupan  939 non-null    object\n",
      " 2   OFF                                                                                        939 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 29.3+ KB\n",
      "None\n",
      "{'NOT', 'OFF'}\n",
      "2999 1000 939  total =  4938\n",
      "4938 4938\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_fire_hasoc_tamil_train = pd.read_excel(\"../data/tamil_hasoc/Tamil-Codemixed_offensive_data_Training-Tweet-HL.xlsx\",index_col=0)\n",
    "df_fire_hasoc_tamil_train = df_fire_hasoc_tamil_train.dropna()\n",
    "\n",
    "print(df_fire_hasoc_tamil_train.info())\n",
    "\n",
    "fire_hasoc_tamil_train_sentences = df_fire_hasoc_tamil_train.values[:,0]\n",
    "fire_hasoc_tamil_train_labels = df_fire_hasoc_tamil_train.values[:,1]\n",
    "\n",
    "new_fire_hasoc_tamil_train_labels=[x.upper() for x in fire_hasoc_tamil_train_labels]\n",
    "\n",
    "fire_hasoc_tamil_train_labels = new_fire_hasoc_tamil_train_labels\n",
    "\n",
    "print(set(fire_hasoc_tamil_train_labels))\n",
    "\n",
    "df_fire_hasoc_tamil_test = pd.read_csv(\"../data/tamil_hasoc/Tamil_hasoc_tanglish_test_withlabels.tsv\",sep=\"\\t\")\n",
    "df_fire_hasoc_tamil_test = df_fire_hasoc_tamil_test.dropna()\n",
    "\n",
    "print(df_fire_hasoc_tamil_test.info())\n",
    "fire_hasoc_tamil_test_sentences = df_fire_hasoc_tamil_test.values[:,1]\n",
    "fire_hasoc_tamil_test_labels = df_fire_hasoc_tamil_test.values[:,2]\n",
    "\n",
    "print(set(fire_hasoc_tamil_test_labels))\n",
    "\n",
    "x_train,x_dev,y_train,y_dev = train_test_split(fire_hasoc_tamil_train_sentences,fire_hasoc_tamil_train_labels,test_size=0.25, random_state=42)\n",
    "\n",
    "x_test = fire_hasoc_tamil_test_sentences\n",
    "y_test = fire_hasoc_tamil_test_labels\n",
    "\n",
    "print(len(x_train),len(x_dev),len(x_test),\" total = \",len(x_train)+len(x_dev)+len(x_test))\n",
    "\n",
    "total_sentences = []\n",
    "total_sentences.extend(fire_hasoc_tamil_train_sentences)\n",
    "total_sentences.extend(fire_hasoc_tamil_test_sentences)\n",
    "\n",
    "total_labels = []\n",
    "total_labels.extend(fire_hasoc_tamil_train_labels)\n",
    "total_labels.extend(fire_hasoc_tamil_test_labels)\n",
    "\n",
    "print(len(total_sentences),len(total_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "artistic-somewhere",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4938, 19598)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(total_sentences)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "animal-referral",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "cosine_sim = cosine_similarity(X,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "joint-nowhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_rows_cosine_sim = np.sum(cosine_sim,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "developmental-canvas",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_cosine_sim = {}\n",
    "for i in range(len(y_train)+len(y_dev),sum_rows_cosine_sim.shape[0]):\n",
    "    dict_cosine_sim[i] = sum_rows_cosine_sim[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "unavailable-christmas",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dict = dict(sorted(dict_cosine_sim.items(), key=lambda item: item[1],reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "french-billion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))\n",
    "\n",
    "n_items = take(200, sorted_dict.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "entitled-atlanta",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_key_dissmilar={}\n",
    "\n",
    "for key,value in n_items:\n",
    "    similar_values = cosine_sim[key]\n",
    "    count_dissimilar_labels = 0\n",
    "    count_similar_labels=0\n",
    "    key_label = total_labels[key]\n",
    "    for i in range(len(similar_values)):\n",
    "        if cosine_sim[key][i]>0:\n",
    "            if key_label == total_labels[i]:\n",
    "                count_similar_labels+=1\n",
    "            else:\n",
    "                count_dissimilar_labels+=1\n",
    "    diff = count_dissimilar_labels-count_similar_labels\n",
    "    dict_key_dissmilar[key] = diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "owned-mapping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{4824: 223, 4670: 177, 4774: 168, 4707: 167, 4799: 87, 4550: 87, 4205: 87, 4634: 77, 4821: 29, 4859: 29, 4854: 25, 4835: 23, 4836: 14, 4782: 10, 4899: 4, 4840: 0, 4890: -4, 4467: -14, 4740: -20, 4726: -29, 4885: -31, 4925: -34, 4704: -34, 4884: -35, 4905: -45, 4908: -49, 4043: -54, 4455: -57, 4839: -58, 4616: -72, 4842: -73, 4522: -76, 4601: -79, 4149: -84, 4331: -86, 4880: -93, 4690: -93, 4841: -96, 4900: -102, 4849: -110, 4789: -110, 4904: -126, 4737: -130, 4370: -131, 4259: -131, 4875: -135, 4573: -137, 4258: -138, 4850: -140, 4510: -150, 4911: -151, 4620: -151, 4354: -151, 4400: -151, 4858: -152, 4775: -153, 4903: -155, 4249: -158, 4709: -162, 4681: -164, 4348: -169, 4306: -172, 4753: -174, 4853: -174, 4914: -176, 4068: -176, 4815: -176, 4894: -179, 4877: -186, 4428: -187, 4769: -191, 4920: -195, 4192: -195, 4834: -195, 4645: -196, 4760: -196, 4384: -196, 4257: -199, 4830: -204, 4758: -207, 4916: -207, 4343: -208, 4751: -210, 4865: -215, 4888: -216, 4863: -217, 4763: -222, 4867: -222, 4416: -223, 4122: -224, 4595: -224, 4857: -226, 4594: -226, 4822: -227, 4912: -228, 4745: -230, 4893: -233, 4864: -246, 4697: -246, 4000: -246, 4034: -249, 4870: -250, 4554: -250, 4889: -250, 4784: -251, 4048: -252, 4495: -255, 4728: -259, 4743: -260, 4844: -263, 4927: -263, 4921: -264, 4506: -268, 4639: -268, 4197: -274, 4677: -274, 4362: -277, 4443: -277, 4561: -278, 4785: -280, 4786: -280, 4713: -282, 4278: -283, 4635: -285, 4807: -286, 4800: -286, 4801: -286, 4802: -286, 4803: -286, 4804: -286, 4805: -286, 4806: -286, 4036: -290, 4662: -291, 4599: -291, 4453: -293, 4838: -294, 4651: -299, 4220: -300, 4521: -300, 4906: -304, 4267: -304, 4727: -306, 4444: -307, 4886: -308, 4353: -309, 4876: -310, 4056: -311, 4832: -315, 4787: -320, 4848: -320, 4339: -320, 4837: -326, 4924: -326, 4808: -326, 4902: -332, 4555: -336, 4482: -340, 4641: -346, 4873: -362, 4374: -363, 4533: -365, 4397: -365, 4040: -369, 4895: -374, 4891: -380, 4431: -384, 4387: -386, 4788: -389, 4456: -394, 4109: -407, 4334: -411, 4487: -414, 4691: -415, 4294: -415, 4303: -421, 4589: -442, 4901: -454, 4553: -454, 4268: -457, 4881: -471, 4534: -471, 4346: -481, 4250: -514, 4351: -540, 4761: -541, 4499: -551, 4477: -571, 4410: -596, 4525: -660, 4501: -663, 4508: -665, 4507: -666, 4559: -677, 4367: -724, 4335: -734, 4497: -767, 4366: -832, 4359: -841, 4371: -999}\n"
     ]
    }
   ],
   "source": [
    "sorted_key_dict = dict(sorted(dict_key_dissmilar.items(), key=lambda item: item[1],reverse=True))\n",
    "print(sorted_key_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "commercial-gender",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_key_items = take(50, sorted_key_dict.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "isolated-exclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "dict_key_dissmilar_label_count={}\n",
    "for key,value in n_key_items:\n",
    "    dissimilar_labels=[]\n",
    "    similar_values = cosine_sim[key]\n",
    "    key_label = total_labels[key]\n",
    "    for i in range(len(similar_values)):\n",
    "        if cosine_sim[key][i]>0:\n",
    "            if key_label != total_labels[i]:\n",
    "                dissimilar_labels.append(total_labels[i])\n",
    "    counted = Counter(dissimilar_labels)\n",
    "    most_common_counts = counted.most_common()\n",
    "    dict_key_dissmilar_label_count[key] = most_common_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "herbal-elephant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4824 [('NOT', 1423)]\n",
      "4670 [('NOT', 977)]\n",
      "4774 [('NOT', 935)]\n",
      "4707 [('NOT', 1332)]\n",
      "4799 [('NOT', 866)]\n",
      "4550 [('NOT', 859)]\n",
      "4205 [('NOT', 736)]\n",
      "4634 [('NOT', 1238)]\n",
      "4821 [('OFF', 1134)]\n",
      "4859 [('NOT', 1045)]\n",
      "4854 [('NOT', 1316)]\n",
      "4835 [('OFF', 1016)]\n",
      "4836 [('OFF', 1128)]\n",
      "4782 [('OFF', 1006)]\n",
      "4899 [('NOT', 971)]\n",
      "4840 [('NOT', 1326)]\n",
      "4890 [('NOT', 1441)]\n",
      "4467 [('NOT', 1103)]\n",
      "4740 [('NOT', 1103)]\n",
      "4726 [('NOT', 1208)]\n",
      "4885 [('NOT', 970)]\n",
      "4925 [('OFF', 1039)]\n",
      "4704 [('OFF', 1231)]\n",
      "4884 [('NOT', 1070)]\n",
      "4905 [('NOT', 1462)]\n",
      "4908 [('NOT', 1332)]\n",
      "4043 [('NOT', 1102)]\n",
      "4455 [('NOT', 1326)]\n",
      "4839 [('OFF', 1123)]\n",
      "4616 [('NOT', 1275)]\n",
      "4842 [('OFF', 1162)]\n",
      "4522 [('NOT', 902)]\n",
      "4601 [('NOT', 1221)]\n",
      "4149 [('NOT', 885)]\n",
      "4331 [('NOT', 971)]\n",
      "4880 [('NOT', 1136)]\n",
      "4690 [('NOT', 1142)]\n",
      "4841 [('OFF', 1121)]\n",
      "4900 [('NOT', 1333)]\n",
      "4849 [('NOT', 1053)]\n",
      "4789 [('OFF', 954)]\n",
      "4904 [('NOT', 1105)]\n",
      "4737 [('OFF', 825)]\n",
      "4370 [('NOT', 1101)]\n",
      "4259 [('NOT', 1257)]\n",
      "4875 [('NOT', 1184)]\n",
      "4573 [('OFF', 532)]\n",
      "4258 [('NOT', 1328)]\n",
      "4850 [('NOT', 1225)]\n",
      "4510 [('OFF', 1050)]\n"
     ]
    }
   ],
   "source": [
    "for key,value in dict_key_dissmilar_label_count.items():\n",
    "    print(key,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "mature-valentine",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_data = []\n",
    "qa_labels=[]\n",
    "for key,value in n_key_items:\n",
    "    if value>0:\n",
    "        qa_data.append(total_sentences[key])\n",
    "        qa_labels.append(total_labels[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "opened-priority",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_df = pd.DataFrame(zip(qa_data,qa_labels),columns=['qa_data','qa_labels'])\n",
    "qa_df.to_csv('tamil_hasoc_qa.tsv',sep='\\t',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-immigration",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_env",
   "language": "python",
   "name": "py3_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
